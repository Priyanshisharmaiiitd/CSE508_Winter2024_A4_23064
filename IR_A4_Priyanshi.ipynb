{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Review Summarization using GPT2 [100 Marks]\n",
        "1. Use the Amazon Fine Food Reviews dataset\n",
        "2. Clean and preprocess the ‘Text’ and ‘Summary’ column from the dataset."
      ],
      "metadata": {
        "id": "JnzZT20xMfNN"
      },
      "id": "JnzZT20xMfNN"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbJU7EaoMxia",
        "outputId": "0bb03b98-85b3-4f33-eb90-4965bc6a3599"
      },
      "id": "DbJU7EaoMxia",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e9ab98",
      "metadata": {
        "id": "14e9ab98"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7b0dd2",
      "metadata": {
        "id": "bd7b0dd2",
        "outputId": "fecea95c-6a10-4873-9a52-276b2a85dfba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId          UserId                      ProfileName  \\\n",
              "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
              "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
              "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
              "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
              "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                     1                       1      5  1303862400   \n",
              "1                     0                       0      1  1346976000   \n",
              "2                     1                       1      4  1219017600   \n",
              "3                     3                       3      2  1307923200   \n",
              "4                     0                       0      5  1350777600   \n",
              "\n",
              "                 Summary                                               Text  \n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
              "4            Great taffy  Great taffy at a great price.  There was a wid...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/ir-4/Reviews.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d8bb297",
      "metadata": {
        "id": "9d8bb297",
        "outputId": "95eee813-d199-4e02-d754-d3d9c15b45d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(568454, 10)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f56bfcc",
      "metadata": {
        "id": "2f56bfcc",
        "outputId": "de1ffaf3-7ad8-4f20-e70f-fa4a009b8268"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Id                         0\n",
              "ProductId                  0\n",
              "UserId                     0\n",
              "ProfileName               26\n",
              "HelpfulnessNumerator       0\n",
              "HelpfulnessDenominator     0\n",
              "Score                      0\n",
              "Time                       0\n",
              "Summary                   27\n",
              "Text                       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bdca4cb",
      "metadata": {
        "id": "9bdca4cb"
      },
      "outputs": [],
      "source": [
        "df.dropna(subset=['Summary'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e927f29f",
      "metadata": {
        "id": "e927f29f",
        "outputId": "7778a9a9-6225-4a4f-8752-716b024a58fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Id                         0\n",
              "ProductId                  0\n",
              "UserId                     0\n",
              "ProfileName               26\n",
              "HelpfulnessNumerator       0\n",
              "HelpfulnessDenominator     0\n",
              "Score                      0\n",
              "Time                       0\n",
              "Summary                    0\n",
              "Text                       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9600152",
      "metadata": {
        "id": "c9600152"
      },
      "outputs": [],
      "source": [
        "newdf = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe769a6",
      "metadata": {
        "id": "9fe769a6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925db0e1",
      "metadata": {
        "scrolled": true,
        "id": "925db0e1",
        "outputId": "a1e42837-799f-4c4c-b848-7248498e2708"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['text_pp'] = newdf['Text'].apply(lambda x: x.lower())\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['summary_pp'] = newdf['Summary'].apply(lambda x: x.lower())\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['text_pp'] = newdf['text_pp'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['text_pp'] = newdf['text_pp'].apply(lambda x: word_tokenize(x))\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: word_tokenize(x))\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['text_pp'] = newdf['text_pp'].apply(lambda x: [word for word in x if word not in stop_words])\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:16: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: [word for word in x if word not in stop_words])\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:20: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['text_pp'] = newdf['text_pp'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
            "/var/folders/gn/nb0tgt3d2klc1xrs4_zrm_kr0000gn/T/ipykernel_40389/3391355768.py:21: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
          ]
        }
      ],
      "source": [
        "# Lowercase conversion\n",
        "newdf['text_pp'] = newdf['Text'].apply(lambda x: x.lower())\n",
        "newdf['summary_pp'] = newdf['Summary'].apply(lambda x: x.lower())\n",
        "\n",
        "# Removing special characters\n",
        "newdf['text_pp'] = newdf['text_pp'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "\n",
        "# Tokenization\n",
        "newdf['text_pp'] = newdf['text_pp'].apply(lambda x: word_tokenize(x))\n",
        "newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Removing stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "newdf['text_pp'] = newdf['text_pp'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "newdf['text_pp'] = newdf['text_pp'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
        "newdf['summary_pp'] = newdf['summary_pp'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "688cd9f4",
      "metadata": {
        "id": "688cd9f4",
        "outputId": "fd475112-a08c-451b-dfc4-48e133a4f8ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>ProductId</th>\n",
              "      <th>UserId</th>\n",
              "      <th>ProfileName</th>\n",
              "      <th>HelpfulnessNumerator</th>\n",
              "      <th>HelpfulnessDenominator</th>\n",
              "      <th>Score</th>\n",
              "      <th>Time</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Text</th>\n",
              "      <th>text_pp</th>\n",
              "      <th>summary_pp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>B001E4KFG0</td>\n",
              "      <td>A3SGXH7AUHU8GW</td>\n",
              "      <td>delmartian</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1303862400</td>\n",
              "      <td>Good Quality Dog Food</td>\n",
              "      <td>I have bought several of the Vitality canned d...</td>\n",
              "      <td>[bought, several, vitality, canned, dog, food,...</td>\n",
              "      <td>[good, quality, dog, food]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>B00813GRG4</td>\n",
              "      <td>A1D87F6ZCVE5NK</td>\n",
              "      <td>dll pa</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1346976000</td>\n",
              "      <td>Not as Advertised</td>\n",
              "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
              "      <td>[product, arrived, labeled, jumbo, salted, pea...</td>\n",
              "      <td>[advertised]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>B000LQOCH0</td>\n",
              "      <td>ABXLMWJIXXAIN</td>\n",
              "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1219017600</td>\n",
              "      <td>\"Delight\" says it all</td>\n",
              "      <td>This is a confection that has been around a fe...</td>\n",
              "      <td>[confection, around, century, light, pillowy, ...</td>\n",
              "      <td>[delight, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>B000UA0QIQ</td>\n",
              "      <td>A395BORC6FGVXV</td>\n",
              "      <td>Karl</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1307923200</td>\n",
              "      <td>Cough Medicine</td>\n",
              "      <td>If you are looking for the secret ingredient i...</td>\n",
              "      <td>[looking, secret, ingredient, robitussin, beli...</td>\n",
              "      <td>[cough, medicine]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>B006K2ZZ7K</td>\n",
              "      <td>A1UQRSCLF8GW1T</td>\n",
              "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>1350777600</td>\n",
              "      <td>Great taffy</td>\n",
              "      <td>Great taffy at a great price.  There was a wid...</td>\n",
              "      <td>[great, taffy, great, price, wide, assortment,...</td>\n",
              "      <td>[great, taffy]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Id   ProductId          UserId                      ProfileName  \\\n",
              "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
              "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
              "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
              "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
              "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
              "\n",
              "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
              "0                     1                       1      5  1303862400   \n",
              "1                     0                       0      1  1346976000   \n",
              "2                     1                       1      4  1219017600   \n",
              "3                     3                       3      2  1307923200   \n",
              "4                     0                       0      5  1350777600   \n",
              "\n",
              "                 Summary                                               Text  \\\n",
              "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
              "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
              "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
              "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
              "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
              "\n",
              "                                             text_pp  \\\n",
              "0  [bought, several, vitality, canned, dog, food,...   \n",
              "1  [product, arrived, labeled, jumbo, salted, pea...   \n",
              "2  [confection, around, century, light, pillowy, ...   \n",
              "3  [looking, secret, ingredient, robitussin, beli...   \n",
              "4  [great, taffy, great, price, wide, assortment,...   \n",
              "\n",
              "                   summary_pp  \n",
              "0  [good, quality, dog, food]  \n",
              "1                [advertised]  \n",
              "2              [delight, say]  \n",
              "3           [cough, medicine]  \n",
              "4              [great, taffy]  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "newdf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training\n",
        "1. Initialize a GPT-2 tokenizer and model from Hugging Face.\n",
        "2. Divide the dataset into training and testing (75:25)\n",
        "3. Implement a custom dataset class to prepare the data for training.\n",
        "4. Fine-tune the GPT-2 model on the review dataset to generate summaries.\n",
        "5. Experiment with different hyperparameters such as learning rate, batch size, and\n",
        "number of epochs to optimize the model's performance."
      ],
      "metadata": {
        "id": "TSQX6hoTMkxn"
      },
      "id": "TSQX6hoTMkxn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0af126",
      "metadata": {
        "id": "ea0af126"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14762878",
      "metadata": {
        "id": "14762878"
      },
      "outputs": [],
      "source": [
        "# Initialize GPT-2 tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Initialize GPT-2 model\n",
        "model = GPT2Model.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "uW2tKgMmNXBO"
      },
      "id": "uW2tKgMmNXBO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c352b8d8",
      "metadata": {
        "id": "c352b8d8"
      },
      "outputs": [],
      "source": [
        "class CreatingDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length):\n",
        "        self.text_ = list(data['Cleaned_Text'])\n",
        "        self.summary_ = list(data['Cleaned_Summary'])\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.length = len(self.text_)\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        review_text = self.text_[idx]\n",
        "        summary = self.summary_[idx]\n",
        "\n",
        "        inputs = self.tokenizer(review_text, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "        labels = self.tokenizer(summary, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "\n",
        "        return inputs[\"input_ids\"].squeeze(0), labels[\"input_ids\"].squeeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(newdf, test_size=0.25, random_state=42)\n",
        "\n",
        "# Instantiate datasets and dataloaders with a larger max_length\n",
        "train_dataset = CreatingDataset(train_data, tokenizer, max_length=1024)\n",
        "test_dataset = CreatingDataset(test_data, tokenizer, max_length=1024)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Define data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(\"Training data size:\", len(train_dataset))\n",
        "print(\"Testing data size:\", len(test_dataset))\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "num_epochs = 3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Check for empty tensors\n",
        "        if inputs.size(0) == 0 or labels.size(0) == 0:\n",
        "            continue\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try:\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        except IndexError as e:\n",
        "            print(f\"IndexError occurred: {e}\")\n",
        "            print(f\"Batch {batch_idx}, Input IDs: {inputs}\")\n",
        "            print(f\"Batch {batch_idx}, Labels: {labels}\")\n",
        "            continue"
      ],
      "metadata": {
        "id": "df6G6CkLN41r"
      },
      "id": "df6G6CkLN41r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7cddfaa",
      "metadata": {
        "id": "d7cddfaa"
      },
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38384bba",
      "metadata": {
        "id": "38384bba",
        "outputId": "4b73e33f-0ad7-45bd-91db-fb1a14595f85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_config = GPT2Config.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel(model_config)\n",
        "state_dict = torch.load(\"fine_tuned_gpt2.pth\", map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation\n",
        "After training, compute ROUGE scores on the test set to assess the model's\n",
        "overall performance i.e. compute ROUGE score for every predicted summary vs\n",
        "the actual summary."
      ],
      "metadata": {
        "id": "9WCydgWjMpMv"
      },
      "id": "9WCydgWjMpMv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065b35e6",
      "metadata": {
        "id": "065b35e6"
      },
      "outputs": [],
      "source": [
        "def compute_rouge(hypotheses, references):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_scores = []\n",
        "    rouge2_scores = []\n",
        "    rougeL_scores = []\n",
        "\n",
        "    for hyp, ref in zip(hypotheses, references):\n",
        "        scores = scorer.score(hyp, ref)\n",
        "        rouge1_scores.append(scores['rouge1'])\n",
        "        rouge2_scores.append(scores['rouge2'])\n",
        "        rougeL_scores.append(scores['rougeL'])\n",
        "\n",
        "    rouge1_avg = get_average_score(rouge1_scores)\n",
        "    rouge2_avg = get_average_score(rouge2_scores)\n",
        "    rougeL_avg = get_average_score(rougeL_scores)\n",
        "\n",
        "    return rouge1_avg, rouge2_avg, rougeL_avg\n",
        "\n",
        "def get_average_score(scores):\n",
        "    precision = sum(score.precision for score in scores) / len(scores)\n",
        "    recall = sum(score.recall for score in scores) / len(scores)\n",
        "    fmeasure = sum(score.fmeasure for score in scores) / len(scores)\n",
        "    return precision, recall, fmeasure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377f7734",
      "metadata": {
        "id": "377f7734"
      },
      "outputs": [],
      "source": [
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# review_text = \"I purchased the Mango flavor, and to me it doesn't take like Mango at all.  There is no hint of sweetness, and unfortunately there is a hint or aftertaste almost like licorice.  I've been consuming various sports nutrition products for decades, so I'm familiar and have come to like the taste of the most of the products I've tried.  The mango flavor is one of the least appealing I've tasted.  It's not terrible, but it's bad enough that I notice the bad taste every sip I take.\"\n",
        "# target_text = \"Taste is not so good.\"\n",
        "# input_text = review_text\n",
        "# input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# output = model.generate(input_ids, max_length=50, num_beams=5, early_stopping=True)\n",
        "# generated_summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# # Compute ROUGE scores\n",
        "# hypotheses = [generated_summary]\n",
        "# references = [target_text]\n",
        "# rouge1_avg, rouge2_avg, rougeL_avg = compute_rouge(hypotheses, references)\n",
        "\n",
        "# # Print results\n",
        "# print(\"\\nGiven Review Text:\", review_text)\n",
        "# print(\"Given Summary:\", target_text)\n",
        "# print(\"Generated Summary:\", generated_summary)\n",
        "# print(\"ROUGE Scores\")\n",
        "# print(f\"ROUGE-1: Precision: {rouge1_avg[0]:.2f}, Recall: {rouge1_avg[1]:.2f}, F1-Score: {rouge1_avg[2]:.2f}\")\n",
        "# print(f\"ROUGE-2: Precision: {rouge2_avg[0]:.2f}, Recall: {rouge2_avg[1]:.2f}, F1-Score: {rouge2_avg[2]:.2f}\")\n",
        "# print(f\"ROUGE-L: Precision: {rougeL_avg[0]:.2f}, Recall: {rougeL_avg[1]:.2f}, F1-Score: {rougeL_avg[2]:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8f8d725",
      "metadata": {
        "id": "a8f8d725",
        "outputId": "87180b6a-5442-4e49-ecfb-736ed498ab75"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review Text: I purchased the Mango flavor, and to me it doesn't take like Mango at all.  There is no hint of sweetness, and unfortunately there is a hint or aftertaste almost like licorice.  I've been consuming various sports nutrition products for decades, so I'm familiar and have come to like the taste of the most of the products I've tried.  The mango flavor is one of the least appealing I've tasted.  It's not terrible, but it's bad enough that I notice the bad taste every sip I take.\n",
            "Actual Summary: Taste is not so good.\n",
            "Generated Summary: summarize:\n",
            "I purchased the Mango flavor, and to me it doesn't take like Mango at all.  There is no hint of sweetness, and unfortunately there is a hint or aftertaste almost like licorice.  I've been consuming various sports nutrition products for decades, so I'm familiar and have come to like the taste of the most of the products I've tried.  The mango flavor is one of the least appealing I've tasted.  It's not terrible, but it's bad enough that I notice the bad taste every sip I take.  I'm not sure if it's because of the mango flavor, or if it's because of the mango flavor.  I'm not sure if it's\n",
            "\n",
            "ROUGE Scores:\n",
            "ROUGE-1: Precision: 0.05, Recall: 0.60, F1-Score: 0.08\n",
            "ROUGE-2: Precision: 0.00, Recall: 0.00, F1-Score: 0.00\n",
            "ROUGE-L: Precision: 0.05, Recall: 0.60, F1-Score: 0.08\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge import Rouge\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Sample Review Text\n",
        "review_text = \"I purchased the Mango flavor, and to me it doesn't take like Mango at all.  There is no hint of sweetness, and unfortunately there is a hint or aftertaste almost like licorice.  I've been consuming various sports nutrition products for decades, so I'm familiar and have come to like the taste of the most of the products I've tried.  The mango flavor is one of the least appealing I've tasted.  It's not terrible, but it's bad enough that I notice the bad taste every sip I take.\"\n",
        "\n",
        "# Actual Summary\n",
        "actual_summary = \"Taste is not so good.\"\n",
        "\n",
        "input_text = \"summarize:\\n\" + review_text\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "generated_summary_ids = model.generate(input_ids, max_length=150, num_beams=2, temperature=0.7, early_stopping=True)\n",
        "generated_summary = tokenizer.decode(generated_summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge = Rouge()\n",
        "scores = rouge.get_scores(generated_summary, actual_summary)[0]\n",
        "\n",
        "print(\"Review Text:\", review_text)\n",
        "print(\"Actual Summary:\", actual_summary)\n",
        "print(\"Generated Summary:\", generated_summary)\n",
        "print(\"\\nROUGE Scores:\")\n",
        "print(\"ROUGE-1: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(scores['rouge-1']['p'], scores['rouge-1']['r'], scores['rouge-1']['f']))\n",
        "print(\"ROUGE-2: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(scores['rouge-2']['p'], scores['rouge-2']['r'], scores['rouge-2']['f']))\n",
        "print(\"ROUGE-L: Precision: {:.2f}, Recall: {:.2f}, F1-Score: {:.2f}\".format(scores['rouge-l']['p'], scores['rouge-l']['r'], scores['rouge-l']['f']))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}